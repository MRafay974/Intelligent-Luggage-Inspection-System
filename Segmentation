import numpy as np
from sklearn.utils import shuffle
from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay
import matplotlib.pyplot as plt
import tensorflow as tf
import ipyplot
import math
import os
from tensorflow.keras.models import *
from tensorflow.keras.layers import *
from tensorflow.keras.optimizers import *
from tensorflow.keras.losses import *
from tensorflow.keras.preprocessing.image import *


import os
import numpy as np
import cv2
# Define the directory paths
image_dir = "/content/drive/MyDrive/Dip_Project_Data/DIP Data Upload/train"
mask_dir = "/content/drive/MyDrive/Dip_Project_Data/DIP Data Upload/train/annotations"
# Define class names
class_names = ['knife','GUN','shuriken']
mask_folders = ['knife', 'GUN', 'shuriken']
# Initialize lists to hold the images and masks
images = []
masks = []
# Iterate over the classes to load images
for class_name in class_names:
 class_path = os.path.join(image_dir, class_name)
 image_files = os.listdir(class_path)
 for img_file in image_files:
 img_path = os.path.join(class_path, img_file)
 img = cv2.imread(img_path, cv2.IMREAD_COLOR)
 if img is None:
 continue # Skip if the image is not readable
 img = cv2.resize(img, (256, 256))
 images.append(img)
 # Initialize mask with zeros
 mask = np.zeros((256, 256), dtype=np.uint8)
 # Load corresponding masks
 for mask_class in mask_folders:
 mask_path = os.path.join(mask_dir, mask_class, img_file)
 if os.path.exists(mask_path):
 mask_img = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)
 if mask_img is not None:
 mask_img = cv2.resize(mask_img, (256, 256))
 mask = np.maximum(mask, mask_img)
 masks.append(mask)
# Convert lists to numpy arrays
images = np.array(images)
masks = np.array(masks)
print(f"Images shape: {images.shape}")
print(f"Masks shape: {masks.shape}")
# Save arrays as .npy files
np.save("Train_images.npy", images)
np.save("Train_masks.npy", masks)



training_file_path = '/content/drive/MyDrive/Segmentation/Train_images.npy'
training_labels_path = '/content/drive/MyDrive/Segmentation/Train_masks.npy'
testing_file_path = '/content/drive/MyDrive/Segmentation/Test_images.npy'
testing_labels_path = '/content/drive/MyDrive/Segmentation/Test_masks.npy'
training_images, training_masks, testing_images, testing_masks = np.load(training_file_path), 
np.load(training_labels_path), np.load(testing_file_path), np.load(testing_labels_path)
print('Shape of training images and training lables is: ', training_images.shape, ',', 
training_masks.shape)
print('Shape of teting images and testing lables is: ', testing_images.shape, ',', 
testing_masks.shape)



# Define the input layer with size (256, 256, 3)
inputs = Input((256, 256, 3))
# Downsampling path
conv1 = Conv2D(8, 3, activation='relu', padding='same', 
kernel_initializer='he_normal')(inputs)
conv1 = BatchNormalization()(conv1)
conv1 = Conv2D(8, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv1)
batch1 = BatchNormalization()(conv1)
drop1 = Dropout(0.25)(batch1)
pool1 = MaxPooling2D(pool_size=(2, 2))(drop1)
conv2 = Conv2D(16, 3, activation='relu', padding='same', 
kernel_initializer='he_normal')(pool1)
conv2 = BatchNormalization()(conv2)
conv2 = Conv2D(16, 3, activation='relu', padding='same', 
kernel_initializer='he_normal')(conv2)
batch2 = BatchNormalization()(conv2)
drop2 = Dropout(0.25)(batch2)
pool2 = MaxPooling2D(pool_size=(2, 2))(drop2)
conv3 = Conv2D(32, 3, activation='relu', padding='same', 
kernel_initializer='he_normal')(pool2)
conv3 = BatchNormalization()(conv3)
conv3 = Conv2D(32, 3, activation='relu', padding='same', 
kernel_initializer='he_normal')(conv3)
batch3 = BatchNormalization()(conv3)
drop3 = Dropout(0.25)(batch3)
pool3 = MaxPooling2D(pool_size=(2, 2))(drop3)
# Upsampling path
up4 = UpSampling2D(size=(2, 2))(pool3)
up4 = Conv2D(16, 3, activation='relu', padding='same', kernel_initializer='he_normal')(up4)
up4 = BatchNormalization()(up4)
merge4 = concatenate([drop3, up4], axis=3)
conv4 = Conv2D(16, 3, activation='relu', padding='same', 
kernel_initializer='he_normal')(merge4)
conv4 = BatchNormalization()(conv4)
conv4 = Conv2D(16, 3, activation='relu', padding='same', 
kernel_initializer='he_normal')(conv4)
conv4 = BatchNormalization()(conv4)
up7 = UpSampling2D(size=(2, 2))(conv4)
up7 = Conv2D(8, 3, activation='relu', padding='same', kernel_initializer='he_normal')(up7)
up7 = BatchNormalization()(up7)
merge7 = concatenate([drop2, up7], axis=3)
conv7 = Conv2D(8, 3, activation='relu', padding='same', 
kernel_initializer='he_normal')(merge7)
conv7 = BatchNormalization()(conv7)
conv7 = Conv2D(8, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv7)
conv7 = BatchNormalization()(conv7)
up8 = UpSampling2D(size=(2, 2))(conv7)
up8 = Conv2D(8, 3, activation='relu', padding='same', kernel_initializer='he_normal')(up8)
up8 = BatchNormalization()(up8)
merge8 = concatenate([drop1, up8], axis=3)
conv8 = Conv2D(8, 3, activation='relu', padding='same', 
kernel_initializer='he_normal')(merge8)
conv8 = BatchNormalization()(conv8)
conv8 = Conv2D(8, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv8)
conv8 = BatchNormalization()(conv8)
# Single output layer
output = Conv2D(1, 1, activation='sigmoid')(conv8)
# Define the model
my_model = Model(inputs=inputs, outputs=output)
# Compile the model
my_model.compile(optimizer=Adam(lr=1e-4), loss=BinaryCrossentropy(), 
metrics=['binary_accuracy'])
# Print the model summary
my_model.summary()



epochs = 10
my_model_history = my_model.fit(x=training_images, y=training_masks, epochs=epochs, 
initial_epoch=0)


# Save the model as an HDF5 file in google drive
model_path_h5 = '/content/drive/MyDrive/Model-Segmentation/my_model.h5'
my_model.save(model_path_h5)



